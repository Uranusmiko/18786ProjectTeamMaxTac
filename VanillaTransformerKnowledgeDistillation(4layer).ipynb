{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IAjMkmQUhwgz"},"outputs":[],"source":["import os\n","import torch\n","import random\n","from tqdm import tqdm\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3F2OH22hzUT"},"outputs":[],"source":["if not (os.path.exists('/content/ijcnlp_dailydialog.zip') and os.path.exists('/content/ijcnlp_dailydialog')):\n","  print('dowloading...')\n","  !wget http://yanran.li/files/ijcnlp_dailydialog.zip\n","  !unzip /content/ijcnlp_dailydialog.zip\n","else:\n","  print('files already exists')\n","\n","if not (os.path.exists('/content/data/test') and os.path.exists('/content/data/train') and os.path.exists('/content/data/validation')):\n","  !mkdir data\n","  !unzip /content/ijcnlp_dailydialog/validation.zip -d /content/data\n","  !unzip /content/ijcnlp_dailydialog/train.zip -d /content/data\n","  !unzip /content/ijcnlp_dailydialog/test.zip -d /content/data\n","else:\n","  print('files already exists')\n","\n","!rm /content/ijcnlp_dailydialog.zip\n","!rm -r /content/ijcnlp_dailydialog\n","\n","clear_output(wait=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENw9Bvv_aYmg"},"outputs":[],"source":["!pip install transformers==4.38.2\n","clear_output(wait=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBRgxZeZRTYl"},"outputs":[],"source":["from transformers import RobertaModel, RobertaTokenizer\n","encoder = RobertaModel.from_pretrained('roberta-base')\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","clear_output(wait=False)\n","print(tokenizer.pad_token_id)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"Mydrive\")"],"metadata":{"id":"xAby2ySzFCEY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLtasfuWhoBE"},"outputs":[],"source":["class CustomDataset():\n","\n","    def __init__(self, data_path, tokenizer, batchsize, maxlength) -> None:\n","        self.data = open(data_path)\n","        self.tokenizer = tokenizer\n","        self.batchsize = batchsize\n","        self.maxlength = maxlength\n","        self.data = self.custom_dataset()\n","\n","    def custom_dataset(self):\n","        data = []\n","        for line in self.data:\n","            seqs = line.split('__eou__')\n","            seqs = ''.join(seqs)\n","            data.append(seqs)\n","        self.length = len(data)//self.batchsize\n","        return data\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def batch_tokenize(self, texts):\n","        token_ids_batch, token_mask_batch = [], []\n","        # add start token\n","        for text in texts:\n","            encoded = self.tokenizer(text,\n","                      truncation = True,\n","                      padding=\"max_length\",\n","                      max_length = self.maxlength,\n","                      return_tensors=\"pt\")\n","            token_ids_batch.append(encoded)\n","        return token_ids_batch\n","\n","    def load_batch(self, shuffle=True):\n","        data = self.custom_dataset()\n","        if shuffle:\n","            random.shuffle(data)\n","\n","        for i in range(0, len(data), self.batchsize):\n","            batch_texts = data[i:i+self.batchsize]\n","            yield self.batch_tokenize(batch_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpzXzoRJiNbi"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["class Model(nn.Module):\n","    def __init__(self, encoder, vocab_size) -> None:\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder_layer = nn.Linear(768, vocab_size)\n","\n","    def forward(self, x, mask):\n","        embeddings = self.encoder(input_ids=x, attention_mask=mask)\n","        embedding = embeddings.last_hidden_state\n","        return self.decoder_layer(embedding)\n","\n","model = Model(encoder, tokenizer.vocab_size).to(device)"],"metadata":{"id":"2SxYT6v5r9kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","dataset = CustomDataset(\"/content/data/train/dialogues_train.txt\", tokenizer, 4, 512)"],"metadata":{"id":"mkyp0rzIvxCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"RWH1HSfq0NpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_model = Model(encoder, tokenizer.vocab_size).to(device)\n","pretrained_state_dict = torch.load(\"/content/Mydrive/MyDrive/18786 Project/Results/big_model\")\n","teacher_model.load_state_dict(pretrained_state_dict)\n","teacher_model.eval()\n","clear_output(wait=False)"],"metadata":{"id":"b7euPZ6WFPY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","import torch.nn.functional as F\n","\n","from torch.nn import Transformer\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","class EmbeddingLayer(nn.Module):\n","    def __init__(self, d_model, vocab_size, max_lens, device=\"cuda\", dropout=0.1):\n","        super().__init__()\n","        self.layer_norm = nn.LayerNorm(d_model)\n","        self.dropout_layer = nn.Dropout(dropout)\n","        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n","        self.positional_enoc = nn.Parameter(torch.zeros(1, max_lens, d_model)).to(device)\n","\n","    def forward(self, x):\n","        embedding = self.layer_norm(self.embedding_layer(x))\n","        return self.dropout_layer(embedding + self.positional_enoc[:, :x.size(1), :])\n","\n","\n","class Vanilla_Transformer(nn.Module):\n","    def __init__(self, vocab_size, d_model, n_head, dim_feedforward, num_layers, max_lens, device=\"cuda\", dropout=0.1) -> None:\n","        super().__init__()\n","        self.embedding_layer = EmbeddingLayer(d_model, vocab_size, max_lens, device)\n","        encoder_layer = TransformerEncoderLayer(d_model, n_head, dim_feedforward, batch_first=True)\n","        self.encoder = TransformerEncoder(encoder_layer, num_layers)\n","        self.output_layer = nn.Linear(d_model, vocab_size)\n","        self.dropout_layer = nn.Dropout(dropout)\n","        self.device = device\n","\n","    def get_causal_mask(self, seq_len, device):\n","        return Transformer.generate_square_subsequent_mask(sz=seq_len, device=device)\n","\n","    def forward(self, x, src_padding_mask=None):\n","        embedding = self.embedding_layer(x)\n","        mask = self.get_causal_mask(x.size(1), device=self.device)\n","        encoded_embedding = self.encoder(src=embedding, mask=mask,\n","                          src_key_padding_mask=src_padding_mask, is_causal=True)\n","        output = self.output_layer(encoded_embedding)\n","        return self.dropout_layer(output)\n","\n","\n","vocab_size, d_model, n_head, dim_feedforward, num_layers, max_lens = tokenizer.vocab_size, 512, 8, 2048, 4, 512\n","student_model = Vanilla_Transformer(vocab_size, d_model, n_head, dim_feedforward, num_layers, max_lens).to(device)"],"metadata":{"id":"69EpGxPNFeXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation(model, dataset, criterion):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for data_point in tqdm(dataset.load_batch(), total=dataset.length, leave=True):\n","            for j in range(len(data_point)):\n","                if j == 0:\n","                    x, mask = data_point[j]['input_ids'].to(device), data_point[j][\"attention_mask\"].to(device)\n","                else:\n","                    x, mask = torch.cat([x, data_point[j]['input_ids'].to(device)], dim=0), torch.cat([mask, data_point[j][\"attention_mask\"].to(device)], dim=0)\n","\n","            src = x[:, :-1]\n","            tgt = x[:, 1: ]\n","            bool_mask = ~ mask.to(torch.bool)\n","            float_mask = bool_mask.to(torch.float)\n","            float_mask = float_mask.masked_fill(bool_mask, float('-inf'))\n","            output = model(src, float_mask[:, :-1])\n","            loss = criterion(output.transpose(1,2), tgt)\n","            epoch_loss += loss.item()\n","        epoch_loss = epoch_loss/(dataset.length)\n","    return epoch_loss"],"metadata":{"id":"dtwqs8qjS-LB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"1yihPks0UjIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def distillation_loss(student_logits, teacher_logits, temperature=1):\n","    student_probs = torch.nn.functional.softmax(student_logits / temperature, dim=-1)\n","    teacher_probs = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n","    # return torch.nn.functional.kl_div(student_probs.log(), teacher_probs, reduction='batchmean')\n","    return torch.nn.functional.kl_div(teacher_probs.log(), student_probs, reduction='batchmean')\n","\n","epoch = 80\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)\n","dataset = CustomDataset(\"/content/data/train/dialogues_train.txt\", tokenizer, 24, 512)\n","validationset = CustomDataset(\"/content/data/validation/dialogues_validation.txt\", tokenizer, 1, 512)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.5, last_epoch=-1)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, threshold=0.001)\n","# please exploring these parameters: epoch, lr, scheduler(step_size, gamma), (num_layer, d_model, n_head, dim_feedforward), batch_size\n","# size of (num_layer=6, d_model=512, n_head=8, dim_feedforward=2048)->153 MB, please don't test model larger than this one\n","train_loss = []\n","valid_loss = []\n","for i in range(epoch):\n","    e_loss = 0\n","    student_model.train()\n","    # teacher_model.eval()\n","    for data_point in tqdm(dataset.load_batch(), total=dataset.length, leave=True):\n","        optimizer.zero_grad()\n","        for j in range(len(data_point)):\n","            if j == 0:\n","                x, mask = data_point[j]['input_ids'].to(device), data_point[j][\"attention_mask\"].to(device)\n","            else:\n","                x, mask = torch.cat([x, data_point[j]['input_ids'].to(device)], dim=0), torch.cat([mask, data_point[j][\"attention_mask\"].to(device)], dim=0)\n","\n","        src = x[:, :-1]\n","        tgt = x[:, 1: ]\n","        bool_mask = ~ mask.to(torch.bool)\n","        float_mask = bool_mask.to(torch.float)\n","        float_mask = float_mask.masked_fill(bool_mask, float('-inf'))\n","        student_logits = student_model(src, float_mask[:, :-1])\n","        # with torch.no_grad():\n","        #     teacher_logits = teacher_model(src, mask[:, :-1])\n","\n","        primary_loss = criterion(student_logits.transpose(1, 2), tgt)\n","        # distillation_losses = distillation_loss(student_logits, teacher_logits)\n","        loss = primary_loss\n","        loss.backward()\n","        optimizer.step()\n","        e_loss += loss.item()\n","\n","    e_loss = e_loss/(dataset.length)\n","    v_loss = validation(student_model, validationset, criterion)\n","    print(e_loss)\n","    print(v_loss)\n","    print('tgts: {}'.format(tokenizer.decode(tgt[0].tolist())))\n","    print('pred: {}'.format(tokenizer.decode(torch.argmax(student_logits, dim=-1)[0].tolist())))\n","    print(\"epoch: {}\".format(i))\n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(f\"Epoch {i + 1}, Current Learning Rate: {current_lr}\")\n","    train_loss.append(e_loss)\n","    valid_loss.append(v_loss)\n","    scheduler.step(v_loss)"],"metadata":{"id":"TaQD8SqRGDFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# epochs = 10\n","# for i in range(epochs):\n","#     epoch_loss = 0\n","#     for data_point in tqdm(dataset.load_batch(), total=dataset.length, leave=True):\n","#         optimizer.zero_grad()\n","#         for j in range(len(data_point)):\n","#             if j == 0:\n","#                 x, mask = data_point[j]['input_ids'].to(device), data_point[j][\"attention_mask\"].to(device)\n","#             else:\n","#                 x, mask = torch.cat([x, data_point[j]['input_ids'].to(device)], dim=0), torch.cat([mask, data_point[j][\"attention_mask\"].to(device)], dim=0)\n","\n","#         src = x[:, :-1]\n","#         tgt = x[:, 1: ]\n","\n","#         output = model(src, mask[:, :-1])\n","#         loss = criterion(output.transpose(1,2), tgt)\n","#         loss.backward()\n","#         optimizer.step()\n","#         epoch_loss += loss.item()\n","#     print(epoch_loss/dataset.length)"],"metadata":{"id":"_nTYgSzqwvXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Use a style template\n","plt.style.use('classic')\n","\n","# Create the plot\n","plt.plot(train_loss, label=\"Train Loss\", color='blue', linewidth=2, marker='o', markersize=5)\n","plt.plot(valid_loss, label=\"Validation Loss\", color='red', linewidth=2, marker='x', markersize=5)\n","\n","# Add labels and title\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Epochs')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Add a grid\n","plt.grid(True)\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"XXrhqWu4wfUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.style.available"],"metadata":{"id":"5vwhjdrYya8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch = 30\n","student_model = torch.load('/content/Mydrive/MyDrive/18786 Project/Results/student_model.pth')"],"metadata":{"id":"IQUVG3OlzXB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"3D1B4Ew3zfYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def distillation_loss(student_logits, teacher_logits, temperature=1):\n","    student_probs = torch.nn.functional.softmax(student_logits / temperature, dim=-1)\n","    teacher_probs = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n","    # return torch.nn.functional.kl_div(student_probs.log(), teacher_probs, reduction='batchmean')\n","    return torch.nn.functional.kl_div(teacher_probs.log(), student_probs, reduction='none')\n","\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n","optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n","dataset = CustomDataset(\"/content/data/train/dialogues_train.txt\", tokenizer, 4, 512)\n","validationset = CustomDataset(\"/content/data/validation/dialogues_validation.txt\", tokenizer, 1, 512)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, threshold=0.001)\n","trains_loss = []\n","valids_loss = []\n","trains_loss_d = []\n","for i in range(epoch):\n","    e_loss_1 = 0\n","    e_loss_2 = 0\n","    student_model.train()\n","    teacher_model.eval()\n","    for data_point in tqdm(dataset.load_batch(), total=dataset.length, leave=True):\n","        optimizer.zero_grad()\n","        for j in range(len(data_point)):\n","            if j == 0:\n","                x, mask = data_point[j]['input_ids'].to(device), data_point[j][\"attention_mask\"].to(device)\n","            else:\n","                x, mask = torch.cat([x, data_point[j]['input_ids'].to(device)], dim=0), torch.cat([mask, data_point[j][\"attention_mask\"].to(device)], dim=0)\n","\n","        src = x[:, :-1]\n","        tgt = x[:, 1: ]\n","        bool_mask = ~ mask.to(torch.bool)\n","        float_mask = bool_mask.to(torch.float)\n","        float_mask = float_mask.masked_fill(bool_mask, float('-inf'))\n","        student_logits = student_model(src, float_mask[:, :-1])\n","        with torch.no_grad():\n","            teacher_logits = teacher_model(src, mask[:, :-1])\n","\n","        float_mask = bool_mask.to(torch.float)[:, 1: ]\n","        # student_logits = student_logits * float_mask.unsqueeze(2)\n","        # teacher_logits = teacher_logits * float_mask.unsqueeze(2)\n","        distillation_losses = distillation_loss(student_logits, teacher_logits)\n","        # loss = distillation_losses * float_mask.unsqueeze(2)\n","        # loss = loss.sum() / mask.sum()\n","        loss = distillation_losses\n","        loss = loss.mean()\n","        loss.backward()\n","        optimizer.step()\n","        # e_loss_1 += primary_loss.item()\n","        e_loss_2 += loss.item()\n","\n","    # e_loss_1 = e_loss_1/(dataset.length)\n","    e_loss_2 = e_loss_2/(dataset.length)\n","    v_loss = validation(student_model, validationset, criterion)\n","    print(e_loss_2)\n","    print(v_loss)\n","    print('tgts: {}'.format(tokenizer.decode(tgt[0].tolist())))\n","    print('pred: {}'.format(tokenizer.decode(torch.argmax(student_logits, dim=-1)[0].tolist())))\n","    print(\"epoch: {}\".format(i))\n","    current_lr = optimizer.param_groups[0]['lr']\n","    print(f\"Epoch {i + 1}, Current Learning Rate: {current_lr}\")\n","    trains_loss_d.append(e_loss_2)\n","    # trains_loss.append(e_loss_1)\n","    valids_loss.append(v_loss)\n","    scheduler.step(v_loss)"],"metadata":{"id":"Ju3AVd41y4lG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}